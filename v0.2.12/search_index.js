var documenterSearchIndex = {"docs":
[{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Distributed linear algebra frameworks are the backbone for efficient parallel codes in data analytics, scientific computing and machine learning. The central idea is that vectors and matrices can be partitioned into potentially overlapping chunks which are distributed across a set of workers on which we define the usual operations like products and norms.","category":"page"},{"location":"usage/#Basic-example","page":"Usage","title":"Basic example","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"In this section we take a look on solving the finite difference discretization of a Laplace problem in 1D over the domain [0,1]. As a reminder, the Laplace problem states to find function u(x) such that Δu(x) = 0 for all x ∈ [0,1]. Without boundary conditions the problem is not well-posed, hence we introduce the Dirichlet condition u(0) = 1.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Applying the finite difference method with length 0.25 we discretize the problem into linear system with 5 unkowns (u₁,...,u₅), which we call degrees of freedom:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"frac14\nbeginpmatrix\n1   0   0   0   0 \n0  -2   1   0   0 \n0   1  -2   1   0 \n0   0   1  -2   1 \n0   0   0   1  -1\nendpmatrix\nbeginpmatrix\nu₁ \nu₂ \nu₃ \nu₄ \nu₅\nendpmatrix\n=\nbeginpmatrix\n 1 \n-1 \n 0 \n 0 \n 0\nendpmatrix","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"A detailed derivation can be found in standard numerical analysis lecture notes and books e.g. these. The linear system is then solved with conjugate gradients.","category":"page"},{"location":"usage/#Commented-Code","page":"Usage","title":"Commented Code","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"To distribute the problem across two workers we have do choose a partitioning. Here we arbitrarily assign the first 3 columns and rows to worker 1 and the remaining 2 rows and columns to worker 2.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"First include the packages which are used.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays, SparseArrays, IterativeSolvers","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"We want a partitioning into 2 pieces and chose the sequential backend to handle the task sequentially so that the code can be executed in a standard Julia REPL (e.g., to simplify debugging).","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"np = 2\nbackend = SequentialBackend()","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Most of the codes using PartitionedArrays start creating a distributed object that for each part contains its part id. We call it parts.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"parts = get_part_ids(backend,np)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now, we generate a partitioning of rows and columns. Note that the entry in row 3 column 4 is visible to the first worker","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"neighbors, row_partitioning, col_partitioning = map_parts(parts) do part\n    if part == 1\n        (\n        Int32[2],\n        IndexSet(part, [1,2,3], Int32[1,1,1]),\n        IndexSet(part, [1,2,3,4], Int32[1,1,1,2])\n        )\n    else\n        (\n        Int32[1],\n        IndexSet(part, [3,4,5], Int32[1,2,2]),\n        IndexSet(part, [3,4,5], Int32[1,2,2])\n        )\n    end\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"We create information exchangers to manage the synchronization of visible shared portions of the sparse matrix and the actual row/col","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"global_number_of_dofs = 5\nrow_exchanger = Exchanger(row_partitioning,neighbors)\nrows = PRange(global_number_of_dofs,row_partitioning,row_exchanger)\n\ncol_exchanger = Exchanger(col_partitioning,neighbors)\ncols = PRange(global_number_of_dofs,col_partitioning,col_exchanger)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next we create the sparse matrix entries in COO format in their worker-local numbering. A note about the exact values of the sparse matrices can be found in the subsection below.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"I, J, V = map_parts(parts) do part\n    if part == 1\n        (\n        [ 1, 1, 2, 2, 2, 3, 3, 3],\n        [ 1, 2, 1, 2, 3, 2, 3, 4],\n        0.25*Float64[1, 0, 0,-2, 1, 1,-1, 0]\n        )\n    else\n        (\n        [ 1, 1, 2, 2, 2, 3, 3],\n        [ 1, 2, 1, 2, 3, 2, 3],\n        0.25*Float64[-1, 1, 1,-2, 1, 1,-1])\n    end\nend\nA = PSparseMatrix(I, J, V, rows, cols, ids=:local)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Since the previous lines created the local prtions we have to trigger sync between the workers.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"assemble!(A)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Construct the right hand side. Note that the first entry of the rhs of worker 2 is shared with worker 1.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"b = PVector{Float64}(undef, A.rows)\nmap_parts(parts,local_view(b, b.rows)) do part, b_local\n    if part == 1\n        b_local .= [1.0, -1.0, 0.0]\n    else\n        b_local .= [0.0, 0.0, 0.0]\n    end\nend","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now the sparse matrix and right hand side of the linear system are assembled globally and we can solve problem with cg. With the end in the last line we close the parallel environment.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"u = IterativeSolvers.cg(A,b)","category":"page"},{"location":"usage/#Parallel-Code","page":"Usage","title":"Parallel Code","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now changing the backend to the MPI backend we can solve the problem in parallel. This just requires to change the line","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backend = SequentialBackend()","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"to","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backend = MPIBackend()","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"and including and initializing MPI. Now launching the script with MPI makes the run parallel.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"$ mpirun -n 2 julia my-script.jl","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Hence the full MPI code is given in the next code box. Note that we have used the with_backend function that automatically includes and initializes MPI for us.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using PartitionedArrays, SparseArrays, IterativeSolvers\n\nnp = 2\nbackend = MPIBackend()\n\nwith_backend(backend,np) do parts\n    # Construct the partitioning\n    neighbors, row_partitioning, col_partitioning = map_parts(parts) do part\n        if part == 1\n            (\n            Int32[2],\n            IndexSet(part, [1,2,3], Int32[1,1,1]),\n            IndexSet(part, [1,2,3,4], Int32[1,1,1,2])\n            )\n        else\n            (\n            Int32[1],\n            IndexSet(part, [3,4,5], Int32[1,2,2]),\n            IndexSet(part, [3,4,5], Int32[1,2,2])\n            )\n        end\n    end\n\n    global_number_of_dofs = 5\n\n    row_exchanger = Exchanger(row_partitioning,neighbors)\n    rows = PRange(global_number_of_dofs,row_partitioning,row_exchanger)\n\n    col_exchanger = Exchanger(col_partitioning,neighbors)\n    cols = PRange(global_number_of_dofs,col_partitioning,col_exchanger)\n\n    # Construct the sparse matrix\n    I, J, V = map_parts(parts) do part\n      if part == 1\n          (\n          [ 1, 1, 2, 2, 2, 3, 3, 3],\n          [ 1, 2, 1, 2, 3, 2, 3, 4],\n          0.25*Float64[1, 0, 0,-2, 1, 1,-1, 0]\n          )\n      else\n          (\n          [ 1, 1, 2, 2, 2, 3, 3],\n          [ 1, 2, 1, 2, 3, 2, 3],\n          0.25*Float64[-1, 1, 1,-2, 1, 1,-1])\n      end\n    end\n    A = PSparseMatrix(I, J, V, rows, cols, ids=:local)\n    assemble!(A)\n\n    # Construct the dense right hand side\n    b = PVector{Float64}(undef, A.rows)\n    map_parts(parts,local_view(b, b.rows)) do part, b_local\n        if part == 1\n            b_local .= [1.0, -1.0, 0.0]\n        else\n            b_local .= [0.0, 0.0, 0.0]\n        end\n    end\n\n    # Solve the linear problem\n    u = IterativeSolvers.cg(A,b)\nend","category":"page"},{"location":"usage/#Note-on-Local-Matrices","page":"Usage","title":"Note on Local Matrices","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"It should be noted that the local matrices are constructed as if they were locally assembled on a process without knowledge of the remaining processes. Dropping the coefficient 0.25 the global and local matrices look as follows:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"     Global Matrix\n   P1  P1  P1  P2  P2\nP1  1   0   0   0   0\nP1  0  -2   1   0   0\nP1  0   1  -2   1   0\nP2  0   0   1  -2   1\nP2  0   0   0   1  -1\n\n           =\n\n   Process 1 Portion\n   P1  P1  P1  P2  P2\nP1  1   0   0   0   0\nP1  0  -2   1   0   0\nP1  0   1  -1   0   0\nP2  x   x   x   x   x\nP2  x   x   x   x   x\n\n          +\n\n   Process 2 Portion\n   P1  P1  P1  P2  P2\nP1  x   x   x   x   x\nP1  x   x   x   x   x\nP1  0   0  -1   1   0\nP2  0   0   1  -2   1\nP2  0   0   0   1  -1","category":"page"},{"location":"usage/#Advanced-example","page":"Usage","title":"Advanced example","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"A more complex example can be found in the package PartitionedPoisson.jl, which describes the assembly of the finite element discretization of a Poisson problem in 3D.","category":"page"},{"location":"reference/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"reference/","page":"API Reference","title":"API Reference","text":"PSparseMatrix\nasync_assemble!\nassemble!\nadd_gids!\nmap_parts","category":"page"},{"location":"reference/#PartitionedArrays.PSparseMatrix","page":"API Reference","title":"PartitionedArrays.PSparseMatrix","text":"PSparseMatrix(init, I, J, V, rows::PRange, cols::PRange, args...; ids, kwargs...)\n\nCreate a new PSparseMatrix from the COO-vectors I, J, and V using init as the initialization method (e.g. sparse for constructing SparseMatrixCSCs on every process, or sparsecsr for constructing SparseMatrixCSR on every process). I, J, and V, should all be AbstractPData-wrapped arrays containing the row-IDs, column-IDs, and values, respectively. rows and cols should be PRanges describing the process ownership of the rows and columns. If the IDs (in I, and J) are given in global-enumeration this should be specified by passing ids=:global as a keyword argument (the constructor will then internally renumber to local IDs), and if the IDs are given in process-local enumeration this should be specified by passing ids=:local.\n\nPSparseMatrix(init, I, J, V, rows::Int, cols::Int, args...; kwargs...)\n\nSame as above, but with rows and cols given as integers. This method requires IDs in I, and J to be globally enumerated (and thus passing ids=:global).\n\nPSparseMatrix(I, J, V, args...; kwargs...)\n\nSame as the methods above, except providing sparse as the default initialization method.\n\n\n\n\n\n","category":"type"},{"location":"reference/#PartitionedArrays.async_assemble!","page":"API Reference","title":"PartitionedArrays.async_assemble!","text":"async_assemble!(I, J, V, rows::PRange) -> Task\n\nCreate a Task for syncronizing the COO-vectors I, J, and V, i.e. sending the triplet (i, j, v) to the process that owns row i, The row ownership is specified by rows.\n\nThe Task that is returned is not scheduled. To execute the assembly use schedule and wait.\n\nExample\n\nassembly_task = async_assemble!(I, J, V, rows)\nmap_parts(schedule, assembly_task)\n# Could do other work here while the task is finishing\nmap_parts(wait, assembly_task)\n\n\n\n\n\n","category":"function"},{"location":"reference/#PartitionedArrays.assemble!","page":"API Reference","title":"PartitionedArrays.assemble!","text":"assemble!(args...; kwargs...)\n\nBlocking version of async_assemble!.\n\n\n\n\n\n","category":"function"},{"location":"reference/#PartitionedArrays.add_gids!","page":"API Reference","title":"PartitionedArrays.add_gids!","text":"add_gids!(a::PRange, gids::AbstractPData, neighbors_snd=nothing, neighbors_rcv=nothing; kwargs...)\n\nFor each part, associate the global IDs in gids to the PRange a. Ghost IDs are added for the global IDs that are not owned by the corresponding part.\n\nSee also add_gids.\n\n\n\n\n\n","category":"function"},{"location":"reference/#PartitionedArrays.map_parts","page":"API Reference","title":"PartitionedArrays.map_parts","text":"map_parts(f, xs::AbstractPData...) -> AbstractPData\n\nTransform the partitioned data in xs by applying f to the underlying local parts, resulting in another partitioned data. If f returns an iterable object, the resulting partitioned data will contain an iterable object at each part. This object can be unpacked into several partitioned data of a single local element.\n\nExample\n\nusing PartitionedArrays\nbackend = SequentialBackend()\np = get_part_ids(backend,3)\nx,y = map_parts(p) do p\n    (sin(p),cos(p))\nend\nmap_parts(x,p) do x,p\n    @assert x == sin(p)\nend\nz = map_parts(+,x,y)\nmap_parts(p,z) do p,z\n    @assert z == sin(p)+cos(p)\nend\n\n\n\n\n\n","category":"function"},{"location":"#PartitionedArrays.jl","page":"Home","title":"PartitionedArrays.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the documentation for PartitionedArrays.jl!","category":"page"},{"location":"","page":"Home","title":"Home","text":"This page is under construction and currently only contains reference docstrings for the API. See the repository README for more information.","category":"page"}]
}
